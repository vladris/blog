<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Ingesting Data &mdash; Blog</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<link rel="stylesheet" href="../../../static/light.css" type="text/css">
<link rel="stylesheet" href="../../../static/pygments.css" type="text/css">
<link rel="shortcut icon" href="../../../static/icon.ico" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<header><span>March 12, 2021</span></header>
<article>
<h1>Ingesting Data</h1>

<p>This is an excerpt from chapter 2 of my book, <a href="https://www.manning.com/books/azure-data-engineering">Data Engineering on
Azure</a>, which
deals with storage. In this article we&#39;ll look at a few aspects of data
ingestion: frequency and load type, and how we can handle corrupted
data. We&#39;ll use Azure Data Explorer as the storage solution, but keep
in mind that the same concepts apply regardless of the data fabric used.
Code samples are omitted from this article, though available in the
book. Let&#39;s start by looking at the frequency with which we ingest
data.</p>

<h2>Ingestion frequency</h2>

<p>Frequency defines how often we ingest a given dataset. This can range
from continuous ingestion, for streaming data, to yearly ingestion - a
dataset which we only need to ingest once a year. For example, our
website team produces web telemetry which we can, if we want to, ingest
in real time. If our analytics scenarios include some real time or near
real time processing, we can bring the data into our data platform as it
is being generated.</p>

<p>The following figure shows this streaming ingestion setup.</p>

<p><img src="fig1.png" alt="image"></p>

<p><em>As users visit website pages, each visit is sent as an event to an
Azure Event Hub. Azure Data Explorer ingests data into the PageViews
table in real time.</em></p>

<p>Azure Event Hub is a service that can receive and process millions of
events per second. An event contains some data payload sent by the
client to the Event Hub. A high-traffic website could treat each page
view as an event and pass the user ID, URL, and timestamp to an Even
Hub. From there, data can be routed to various other services. In our
case, it can be ingested in Azure Data Explorer in real time.</p>

<p>Another option, if we don&#39;t have any real time requirements, is to
ingest the data on some regular cadence, for example every midnight we
load the logs for the day.</p>

<p>The following figure shows this alternative setup.</p>

<p><img src="fig2.png" alt="image"></p>

<p><em>Logs get copied from the website Azure Data Explorer cluster to our
Azure Data Explorer cluster using an Azure Data Factory. Copy happens on
a daily basis.</em></p>

<p>In this case, the website team stores its logs into a dedicated Azure
Data Explorer cluster. Their cluster only stores data for the past 30
days since it is used just to measure the website performance and debug
issues. Since we want to keep data for longer for analytics, we want to
copy it to our cluster and preserve it there.</p>

<p>Azure Data Factory is the Azure ETL service, which enables serverless
data integration and transformation. We can use a Data Factory to
coordinate when and where data gets moved. In our case, we copy the logs
of the previous day every night and append them to our <code>PageViews</code>
table.</p>

<p>Let&#39;s take another example: the sales data from our Payments team. We
use this data to measure revenue and other business metrics. Since not
all transactions are settled, it doesn&#39;t make sense to ingest this data
daily. Our Payments team curates this data and officially publishes the
financials for the previous month on the first day of each month. This
is an example of a monthly dataset, one we would ingest once it becomes
available, on the 1st of each month.</p>

<p>The following figure shows this ingestion.</p>

<p><img src="fig3.png" alt="image"></p>

<p><em>Sales data gets copied from the Payments team&#39;s Azure SQL to our Azure
Data Explorer cluster on a monthly cadence.</em></p>

<p>This is very similar to our previous Azure Data Factory ingestion of
page view logs, the difference being the data source - in this case we
ingest data from Azure SQL, and the ingestion cadence - monthly instead
of daily.</p>

<p>Let&#39;s define the cadence of when a dataset is ready for ingestion as
its <em>grain</em>.</p>

<blockquote>
<p>The <strong>grain</strong> of a dataset specifies the frequency at which new data
is ready for consumption. This can be continuous for streaming data,
hourly, daily, weekly, and so on.</p>
</blockquote>

<p>We would ingest a dataset with a weekly grain on a weekly cadence. The
grain is usually defined by the upstream team producing the dataset.
Partial data might be available earlier, but the upstream team can
usually tell us when the dataset is complete and ready to be ingested.</p>

<p>While some data, like the logs in our example, can be ready in real time
or at a daily grain, there are datasets who get updated once a year. For
example, businesses use fiscal years for financial reporting, budgeting
and so on. These datasets only change year over year.</p>

<p>Another ingestion parameter is the type of data load.</p>

<h2>Load type</h2>

<p>Outside of streaming data, where data gets ingested as it is produced,
we have two options for updating a dataset in our system. We can perform
a full load or an incremental load.</p>

<blockquote>
<p>A <strong>full load</strong> means we fully refresh the dataset, discarding our
current version and replacing it with a new version of the data.</p>
</blockquote>

<p>For example, our Customer Success team has the list of active customer
issues. As these issues get resolved and new issues appear, we perform a
full load whenever we ingest the active issues into our system.</p>

<p>The usual pattern is to ingest the updated data into a staging table,
then swap it with the destination table, as show in the following
figure.</p>

<p><img src="fig4.png" alt="image"></p>

<p><em>Queries are running against the ActiveIssues table. We ingest the data
into the ActiveIssuesStaging table. Queries are still running against
the old ActiveIssues table. We swap the two tables. Queries already
started before the swap will run against the old tables, queries started
after the swap will run against the new table. Finally, we can drop the
old table.</em></p>

<p>Most storage solutions offer some transactional guarantees on renames to
support scenarios like this. This means if someone is running a query
against the <code>ActiveIssues</code> table, there is no chance of the query
failing due to the table not being found or of the query getting rows
from both the old and the new table. Queries running in parallel with a
rename are guaranteed to either hit the old or the new table.</p>

<p>The other type of data load is incremental.</p>

<blockquote>
<p>An <strong>incremental load</strong> means we append data to the dataset. We start
with the current version and enhance it with additional data.</p>
</blockquote>

<p>Let&#39;s take as an example a <code>PageViews</code> table. Since the Website team
only keeps logs around for 30 days and we want to maintain a longer
record when we ingest the data into our system, we can&#39;t fully refresh
the <code>PageViews</code> table. Instead, every night we take the page view logs
of the previous day and we <em>append</em> them to the table.</p>

<p>One challenge of incremental loads is to figure out exactly what data is
missing (that we need to append), and what data we already have. We
don&#39;t want to append again data we already have, as it would create
duplicates.</p>

<p>There are a couple of ways we can go about determining the delta between
upstream and our storage. The simplest one is contractual: the upstream
team guarantees that data will be <q>ready</q> at a certain time or date.
For example, the Payments team promises that the sales data for the
previous month will be ready on the 1st, by noon. In that case, on July
1st we will load all sales data with a timestamp within June and append
it to the existing sales data we have in our system. In this case, the
delta is June sales.</p>

<p>Another way to determine the delta is to keep track on our side of what
is the last row we ingested and only ingest from upstream data after
this row. This is also known as a <em>watermark</em>. Whatever is <q>under the
watermark</q> is data we already have in our system. Upstream can have
data <q>above the watermark</q>, which we need to ingest.</p>

<p>Depending on the dataset, keeping track of the watermark can be very
simple or very complex. In the simplest case, if the data has a column
where values always increase, we can simply see what the latest value is
in our dataset and ask upstream for data with values greater than our
latest.</p>

<p>We can then ask for page views with a timestamp greater than the
watermark when we append data in our system.</p>

<p>Other examples of ever-increasing values are auto-incrementing columns,
like the ones we can define in SQL.</p>

<p>Things get more complicated if there is no easy ordering of the data
from which we can determine our watermark. In that case, the upstream
system needs to keep track of what data it already gave us, and hand us
a watermark object. When we hand back the object, upstream can determine
what is the delta we need. Fortunately, this scenario is less common in
the big data world. We usually have simpler ways to determine delta,
like timestamps and auto-incrementing IDs.</p>

<p>What happens though when a data issue makes its way into the system? We
got the sales data from our Payments team on July 1st, but the next day
we get notified that there was an issue: somehow a batch of transactions
was missing. They fixed the dataset upstream, but we already loaded the
erroneous data into our platform. Let&#39;s talk about restatements and
reloads.</p>

<h2>Restatements and reloads</h2>

<p>In a big data system, it is inevitable that at some point, some data
gets corrupted, or is incomplete. The owners of the data fix the
problem, then issue a <em>restatement</em>.</p>

<blockquote>
<p>A <strong>restatement</strong> of a dataset is a revision and re-release of a
dataset after one or more issues were identified and fixed.</p>
</blockquote>

<p>Once data is restated, we need to reload it into our data platform. This
is obviously much simpler if we perform a full load for the dataset. In
that case, we simply discard the corrupted data we previously loaded and
replace it with the restated data.</p>

<p>Things get more complicated if we load this dataset incrementally. In
that case, we need to drop only the corrupted slice of the data and
reload that from upstream. Let&#39;s see how we can do this in Azure Data
Explorer .</p>

<p>Azure Data Explorer stores data in <em>extents</em>. An extent is a shard of
the data, a piece of a table which contains some of its rows. Extents
are immutable - once written, they are never modified. Whenever we
ingest data, one or more extents are created. Periodically, Azure Data
Explorer merges extents to improve query performance. This is handled by
the engine in the background.</p>

<p>The following figure shows how extents are created during ingestion,
then merged by Azure Data Explorer.</p>

<p><img src="fig5.png" alt="image"></p>

<p><em>Extents are created during ingestion, then merged by Azure Data
Explorer to improve query performance</em></p>

<p>While we can&#39;t modify an extent, we can drop it. Dropping an extent
removes all data stored within. Extents support tagging, which enable us
to attach metadata to them. A best practice is to add the <code>drop-by</code> tag
to extents on creation. This tag has special meaning for Azure Data
Explorer: it will only merge extents with the same <code>drop-by</code> tag. This
will ensure that all data ingested into an extent with a <code>drop-by</code> tag
is never grouped with data ingested with another <code>drop-by</code> tag.</p>

<p>The following figure shows how we can use this tag to ensure data
doesn&#39;t get mixed, then we can drop extents with that tag to remove
corrupted data.</p>

<p><img src="fig6.png" alt="image"></p>

<p><em>We ingested 2 extents with drop-by tag 2020-06-29 and 2 extents with
drop-by tag 2020-06-30. They get merged into 1 extent with drop-by tag
2020-06-29 and 1 extent with drop-by tag 2020-06-30. We can ask Azure
Data Explorer to drop all extents tagged with 2020-06-29 to remove a
part of the data.</em></p>

<p>The <code>drop-by</code> tag ensures that extents with different values for the tag
never get merged together, so we don&#39;t risk dropping more data than
what we want dropped. The value of the tag is arbitrary, we can use
anything, but a good practice is to use an ingestion timestamp. So for
example when we load data on 2020-06-29, we use the <code>drop-by:2020-06-29</code>
tag.</p>

<p>If we later learn that the data we loaded was corrupted and upstream
restates the data, we can drop the extents containing corrupted data and
re-ingest from upstream to repair our dataset.</p>

<p>Obviously, this process is more complicated than if we were doing a full
load of the data every time. In general, if we can afford a full load,
we should use that. Maintenance-wise, it is a much simpler approach.
Sometimes though, this is impossible - for example if we want to
maintain page view logs beyond the 30-day retention period upstream has,
we can&#39;t keep reloading the data. Other times, full load is just too
expensive: we end up moving the same gigabytes of data again and again,
with minor differences. For these situations, we have to look at an
incremental load and manage the additional complexity.</p>

<h2>Summary</h2>

<ul>
<li>We can ingest data continuously (streaming), or at a certain regular
cadence like daily, weekly, monthly, or yearly.</li>
<li>When we ingest data, we can perform either a full load, or we can
perform an incremental load.</li>
<li>A full load means we fully refresh the dataset, discarding our
current version and replacing it with a new version of the data.</li>
<li>An incremental load means we append data to the dataset. We start
with the current version and enhance it with additional data.</li>
<li>It is inevitable for some data to get corrupted. Once repaired
upstream, we need a way to discard the corrupted data from our
system and reload the updated data.</li>
</ul>

</article>
<nav>

<div id="prev"><span>« <a href="../../../2020/12/29/recommendations.html">Recommendations</a></span></div>


<div id="next"><span><a href="../../../2021/06/21/data-engineering-on-azure-rtm.html">Data Engineering on Azure RTM</a> »</span></div>

</nav>
<footer><span>By <a href="https://vladris.com">Vlad Rișcuția</a> | <a href="../../../rss.xml">Subscribe</a> | <a href="../../../index.html">Index</a></span></footer>
</body>
</html>