<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Large Language Models at Work RTM &mdash; Blog</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<link rel="stylesheet" href="../../../static/light.css" type="text/css">
<link rel="stylesheet" href="../../../static/pygments.css" type="text/css">
<link rel="shortcut icon" href="../../../static/icon.ico" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<header><span>November 03, 2023</span></header>
<article>
<h1>Large Language Models at Work RTM</h1>

<p>Keeping <a href="https://vladris.com/blog/2019/10/16/programming-with-types-rtm.html">with</a>
<a href="https://vladris.com/blog/2021/06/21/data-engineering-on-azure-rtm.html">tradition</a>,
I&#39;m writing the RTM post for Large Language Models at Work. The book is done.
Now available <a href="https://www.amazon.com/dp/B0CLSSM8RL">on Kindle</a>.</p>

<h2>Self-publishing</h2>

<p>I decided not to contact a publisher this time around, for a couple of reasons:
First, I didn&#39;t want the pressure of a contract and timelines (though looking
back, I did finish this book faster than the previous two); Second, I had no
idea if I will be able to write something that is still valuable by the time
the book is done, considering the speed of innovation. More on this later.</p>

<p>I authored the book in the open, at <a href="https://vladris.com/llm-book/">https://vladris.com/llm-book/</a> and
self-published <a href="https://www.amazon.com/dp/B0CLSSM8RL">on Kindle</a>. Maybe I will
look into making it a print book at some point, for now I&#39;m keeping it digital.</p>

<p>Amazon offers a nice set of tools to import and format ebooks, but they have
some big limitations - for example, no support for formatting tables, footnotes
etc. I also couldn&#39;t convince the tool the code samples should be monospace on
import so I had to manually re-set the font on each. The book has a few
formatting glitches because of these limitations, which make me reluctant to
look into a print book as I expect I will need to do a lot more manual tweaking
for the text to look good in print.</p>

<h2>Speed of innovation</h2>

<p>I mused about this in chapter 10: Closing Thoughts. I&#39;ll repeat it here as it
perfectly highlight why it is impossible to pin down this strange new world of
AI.</p>

<p>I started writing the book in April 2023. When I picked up the project, GPT-4
was in private preview, with GPT-3.5 being the most powerful globally available
model offered by OpenAI. Since then, GPT-4 opened to the public.</p>

<p>In June, OpenAI announced Functions - fortunately, this happened just before I
started working on chapter 6, Interacting with External Systems. Before
Functions, the way to get a large language model to connect with native code was
through few-shot learning in the prompt, covered in the Non-native functions
section. Originally, I was planning to focus exclusively on this implementation.
Of course, built-in support makes it easier to specify available functions and
the model interaction is likely to work better - since the model has been
specifically trained to <q>understand</q> function definitions and output correct
function calls.</p>

<p>In August, OpenAI announced fine-tuning support for <code>gpt-3.5-turbo</code>. When I was
writing the first draft of chapter 4, Learning and Tuning, the only models that
used to support fine-tuning were the older GPT-3 generation models: Ada,
Babbage, Currie, and Davinci. This was particularly annoying, as the quality of
output produced by these models is way below <code>gpt-3.5-turbo</code> levels. Now, with
the newer models having fine-tuning support, I had to rewrite the Fine-tuning
section.</p>

<p><code>text-davinci-003</code> launched in November of 2022, while <code>gpt-3.5-turbo</code> launched
on March 1st 2023. When I started writing the book, <code>text-davinci-003</code> was
backing most large language model-based solutions across the industry, and
migrations to the newer <code>gpt-3.5-turbo</code> were underway. <code>text-davinci-003</code> is
deprecated to be removed by January 4, 2024 (to be replaced by
<code>gpt-3.5-turbo-instruct</code>), and the industry is moving to adopt GPT-4. I had to
update several code samples from <code>text-davinci-003</code> to <code>gpt-3.5-turbo-instruct</code>.</p>

<p>No idea how long the code samples will keep working or when OpenAI will decide
to deprecate <code>gpt-3.5-turbo</code> or introduce an even more powerful model with
capabilities not covered in the book.</p>

<h2>Time(lessness)</h2>

<p>While some of the code examples will not age well as new models and APIs get
release, the underlying principles of working with large language models that I
walked through in this book - prompt engineering, memory, interacting with
external systems, planning, and so on - will be relevant for a while.
Understanding these fundamentals should help anyone ramp up in the space.</p>

<p>This is an exciting new field, that is going to see a lot more innovation in
the near future. But I expect some of these fundamentals to carry on, in one
shape or another. I hope the topics discussed in this book to remain
interesting for long after the specific models used in the examples become
obsolete.</p>

<h2>Excertps</h2>

<p>Like with my previous books, I&#39;ve been publishing excerpts as shorter,
stand-alone reads. This might sound a bit strange in this case, as the book
is already all online. But I figured it will hopefully help reach more
people, and I did some work on each excerpt to remove references to other
parts of the book so they can, indeed, be read wihtout context. I published
all of these on Medium:</p>

<ul>
<li><a href="https://medium.com/@vladris/n-shot-learning-f9bc0d670a41">N-shot Learning</a></li>
<li><a href="https://medium.com/@vladris/embeddings-and-vector-databases-732f9927b377">Embeddings and Vector Databases</a></li>
<li><a href="https://medium.com/@vladris/interacting-with-external-systems-1951e820b2e7">Interacting with External Systems</a></li>
<li><a href="https://medium.com/@vladris/planning-d00cc124868f">Planning</a></li>
<li><a href="https://medium.com/@vladris/adversarial-llm-attacks-17ba03621e61">Adversarial LLM Attacks</a></li>
</ul>

<p>I hope you enjoy the book! Check it out here: <a href="https://www.amazon.com/dp/B0CLSSM8RL">Large Language Models at Work</a>.</p>

</article>
<nav>

<div id="prev"><span>« <a href="../../../2023/06/18/large-language-models-at-work.html">Large Language Models at Work</a></span></div>


<div id="next"><span><a href="../../../2023/11/28/mental-poker-part-3-transport.html">Mental Poker Part 3: Transport</a> »</span></div>

</nav>
<footer><span>By <a href="https://vladris.com">Vlad Rișcuția</a> | <a href="../../../rss.xml">Subscribe</a> | <a href="../../../index.html">Index</a></span></footer>
</body>
</html>